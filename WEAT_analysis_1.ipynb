{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/keras/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import praw\n",
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from collections import Counter\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import PCA\n",
    "from matplotlib import pyplot\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_subreddit(subreddit_name, app_name, oa_script, oa_secret, pwd):\n",
    "    '''\n",
    "    returns subreddit object with top 1000 subreddit posts\n",
    "    \n",
    "    '''\n",
    "\n",
    "    with open(oa_script) as f:\n",
    "    \n",
    "        script = f.read().strip()\n",
    "\n",
    "    with open(oa_secret) as f:\n",
    "    \n",
    "        secret = f.read().strip()\n",
    "\n",
    "    with open(pwd) as f:\n",
    "    \n",
    "        pw = f.read().strip()\n",
    "    \n",
    "# get reddit object\n",
    "    \n",
    "    reddit = praw.Reddit(client_id=script,\n",
    "                        client_secret=secret,\n",
    "                        user_agent=app_name,\n",
    "                        username='neurodivergent_ai',\n",
    "                        password=pw)\n",
    "\n",
    "    # get subreddit\n",
    "\n",
    "    subreddit = reddit.subreddit(subreddit_name)\n",
    "\n",
    "    # get top posts\n",
    "\n",
    "    top = subreddit.top(limit=1000)\n",
    "\n",
    "    for post in subreddit.top(limit=5):\n",
    "    \n",
    "        print(post.title, '\\n')\n",
    "        \n",
    "    return top\n",
    "\n",
    "\n",
    "def make_subreddit_df(subreddit_object):\n",
    "    '''\n",
    "    takes subreddit api object,\n",
    "    returns as dataframe:\n",
    "    \n",
    "    * entire subreddit, all columns\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    columns_dict = {'title': [],\n",
    "                   'url': [],\n",
    "                   'date': [],\n",
    "                   'score': [],\n",
    "                   'n_comments': [],\n",
    "                   'body': []}\n",
    "    \n",
    "    # build dictionary\n",
    "    \n",
    "    for post in subreddit_object:\n",
    "        \n",
    "        columns_dict['title'].append(post.title)\n",
    "        \n",
    "        columns_dict['url'].append(post.url)\n",
    "        \n",
    "        columns_dict['date'].append(post.created)\n",
    "        \n",
    "        columns_dict['score'].append(post.score)\n",
    "        \n",
    "        columns_dict['n_comments'].append(post.num_comments)\n",
    "        \n",
    "        columns_dict['body'].append(post.selftext)\n",
    "    \n",
    "    # convert to dataframe\n",
    "    \n",
    "    subreddit_df = pd.DataFrame(columns_dict)\n",
    "    \n",
    "    return subreddit_df\n",
    "\n",
    "# text cleaning: 2 different versions\n",
    "# light clean for model training, heavy clean for vocab work\n",
    "\n",
    "def light_clean(doc):\n",
    "    '''\n",
    "    lightly cleans input doc for\n",
    "    word embedding training:\n",
    "    \n",
    "    * tokenizes\n",
    "    * lowercase\n",
    "    * removes punctuation & non-alpha characters\n",
    "    \n",
    "    returns clean document.\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    # tokenize\n",
    "    \n",
    "    split_tokens = doc.split()\n",
    "    \n",
    "    # punctuation list\n",
    "    \n",
    "    punctuation = string.punctuation\n",
    "    \n",
    "    # set up regex filter\n",
    "    \n",
    "    rgx_punct_filter = re.compile('[%s]' % re.escape(punctuation))\n",
    "    \n",
    "    # apply filter\n",
    "    \n",
    "    no_punct_tokens = [rgx_punct_filter.sub('', char) for char in split_tokens]\n",
    "    \n",
    "    # alpha tokens only\n",
    "    \n",
    "    alpha_tokens = [t for t in no_punct_tokens if t.isalpha()]\n",
    "    \n",
    "    # lowercase\n",
    "    \n",
    "    lower_tokens = [t.lower() for t in alpha_tokens]\n",
    "    \n",
    "    return lower_tokens\n",
    "\n",
    "def super_clean(doc):\n",
    "    '''\n",
    "    cleans a single input document.\n",
    "    preps for vocabulary analysis.\n",
    "    applies more processing than\n",
    "    light_clean():\n",
    "    \n",
    "    * tokenizes\n",
    "    * lowercase\n",
    "    * removes punctuation & numbers\n",
    "    * removes stopwords\n",
    "    \n",
    "    returns clean document.\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    # tokenize\n",
    "    \n",
    "    split_tokens = doc.split()\n",
    "    \n",
    "    # grab punctuation list\n",
    "    \n",
    "    punctuation = string.punctuation\n",
    "    \n",
    "    # setup re filter\n",
    "    \n",
    "    rgx_punct_filter = re.compile('[%s]' % re.escape(punctuation))\n",
    "    \n",
    "    # apply filter\n",
    "    \n",
    "    no_punct_tokens = [rgx_punct_filter.sub('', char) for char in split_tokens]\n",
    "    \n",
    "    # take out numbers & any other non-alpha characters\n",
    "    \n",
    "    alpha_tokens = [i for i in no_punct_tokens if i.isalpha()]\n",
    "    \n",
    "    # lowercase\n",
    "    \n",
    "    lower_tokens = [t.lower() for t in alpha_tokens]\n",
    "    \n",
    "    # get stop words\n",
    "    \n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    \n",
    "    # filter out stopwords\n",
    "    \n",
    "    go_tokens = [t for t in lower_tokens if not t in stop_words]\n",
    "    \n",
    "    # remove very short tokens\n",
    "    \n",
    "    clean_tokens = [word for word in go_tokens if len(word) > 1]\n",
    "    \n",
    "    return clean_tokens\n",
    "\n",
    "def clean_all_docs(docs):\n",
    "    '''\n",
    "    takes a set of documents,\n",
    "    applies light_clean() and \n",
    "    returns as a list\n",
    "    '''\n",
    "    clean_docs = []\n",
    "    \n",
    "    for doc in docs:\n",
    "        \n",
    "        doc = light_clean(doc)\n",
    "        \n",
    "        if doc:\n",
    "            \n",
    "            clean_docs.append(doc)\n",
    "    \n",
    "    return clean_docs\n",
    "\n",
    "\n",
    "def train_w2v_model(docs):\n",
    "    '''\n",
    "    trains word2vec model on input set of docs.\n",
    "    fits PCA projection to 2d space.\n",
    "    prints 2d visualization of words in vector space.\n",
    "    returns trained model.\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    # train w2v model \n",
    "    \n",
    "    w2v_model = Word2Vec(docs, min_count=1)\n",
    "    \n",
    "    # get vocab\n",
    "    \n",
    "    vector_vocab = w2v_model[w2v_model.wv.vocab]\n",
    "    \n",
    "    # fit PCA model / 2d projection\n",
    "    \n",
    "    pca_model = PCA(n_components=2)\n",
    "    \n",
    "    pca_projection = pca_model.fit_transform(vector_vocab)\n",
    "    \n",
    "    # plot\n",
    "    \n",
    "    pyplot.scatter(pca_projection[:, 0], pca_projection[:, 1])\n",
    "    \n",
    "    # plot with words\n",
    "    \n",
    "    vocab_list = list(w2v_model.wv.vocab)\n",
    "    \n",
    "    for i, word in enumerate(vocab_list):\n",
    "        \n",
    "        pyplot.annotate(word, xy=(pca_projection[i, 0], pca_projection[i, 1]))  \n",
    "    \n",
    "    return w2v_model\n",
    "\n",
    "    \n",
    "def get_cos_sim(model, word_1, word_2):\n",
    "    \n",
    "    return model.wv.similarity(word_1, word_2)\n",
    "\n",
    "\n",
    "def compare_cos_sim(model, word_list, comp_word):\n",
    "    '''\n",
    "    retrieves cosine similarities for a word\n",
    "    vs a list of words, returns list\n",
    "    '''\n",
    "    \n",
    "    cos_sim_list = []\n",
    "    \n",
    "    for word in word_list:\n",
    "        \n",
    "        cos_sim = get_cos_sim(model, word, comp_word)\n",
    "        \n",
    "        in_list = []\n",
    "        \n",
    "        in_list.append(comp_word)\n",
    "        \n",
    "        in_list.append(word)\n",
    "        \n",
    "        in_list.append(cos_sim)\n",
    "        \n",
    "        cos_sim_list.append(in_list)\n",
    "    \n",
    "    return cos_sim_list    \n",
    "\n",
    "\n",
    "def comp_cos_sim_lists(model, list_1, list_2):\n",
    "    '''\n",
    "    compares cosine similarities of terms\n",
    "    on 2 lists. returns a list.\n",
    "    '''\n",
    "    \n",
    "    cos_sim_list = []\n",
    "    \n",
    "    for word in list_1:\n",
    "        \n",
    "        if word not in list_2:\n",
    "            \n",
    "            sim_line = compare_cos_sim(model, list_2, word)\n",
    "            \n",
    "            cos_sim_list.append(sim_line)\n",
    "    \n",
    "    return cos_sim_list\n",
    "\n",
    "\n",
    "def join_corpus(df):\n",
    "    \n",
    "    corpus = list(df['title']) + list(df['body'])\n",
    "    \n",
    "    return corpus\n",
    "\n",
    "\n",
    "def get_vocab(docs):\n",
    "    '''\n",
    "    vocabulary processing\n",
    "    applies super_clean() text prep\n",
    "    removes stopwords\n",
    "    returns vocabulary as DataFrame, \n",
    "    total training words, \n",
    "    unique vocabulary words.\n",
    "    '''\n",
    "    \n",
    "    words = []\n",
    "    \n",
    "    for doc in docs:\n",
    "        \n",
    "        # super_clean() removes stopwords\n",
    "        # returns a more useful vocabulary for analysis\n",
    "        \n",
    "        doc = super_clean(doc)\n",
    "        \n",
    "        for word in doc:\n",
    "            \n",
    "            words.append(word)\n",
    "    \n",
    "    total_words = len(words)\n",
    "    \n",
    "    word_counts = Counter(words)\n",
    "    \n",
    "    unique_words = len(word_counts)\n",
    "    \n",
    "    word_counts = pd.DataFrame.from_dict(word_counts, orient='index')\n",
    "    \n",
    "    # return df sorted by word frequency\n",
    "    \n",
    "    return word_counts.sort_values(by=[0], ascending=False), total_words, unique_words\n",
    "\n",
    "\n",
    "def list_sims_for_mean(sim_comp_list):\n",
    "    '''\n",
    "    takes list of cosine similarity comparisons with words\n",
    "    returns ordered list of numerical values only\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    sims_for_mean = []\n",
    "    \n",
    "    for sub_list in sim_comp_list:\n",
    "        \n",
    "        for i in sub_list:\n",
    "            \n",
    "            # grab 3rd item/cos sim\n",
    "            \n",
    "            sim = i[2]\n",
    "        \n",
    "            sims_for_mean.append(sim)\n",
    "    \n",
    "    return sims_for_mean\n",
    "\n",
    "        \n",
    "def get_sim_means(sims_for_mean_1, sims_for_mean_2, sims_for_mean_3):\n",
    "    '''\n",
    "    takes 3 ordered lists of numerical values\n",
    "    takes the mean & appends each mean to a list\n",
    "    returns an ordered list of mean cosine similarities\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    sim_means_list = []\n",
    "    \n",
    "    for i in range(len(sims_for_mean_1)):\n",
    "        \n",
    "        sim_mean = (sims_for_mean_1[i] + sims_for_mean_2[i] + sims_for_mean_3[i]) / 3\n",
    "        \n",
    "        sim_means_list.append(sim_mean)\n",
    "    \n",
    "    return sim_means_list\n",
    "    \n",
    "\n",
    "def get_mean_cos_sim(sim_list_1, sim_list_2, sim_list_3):\n",
    "    '''\n",
    "    takes 3 lists of cosine similarity comparisons with words\n",
    "    gets mean cosine similarities for each term\n",
    "    returns new list with terms & mean cosine sims for each term\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    sims_1 = list_sims_for_mean(sim_list_1)\n",
    "    \n",
    "    sims_2 = list_sims_for_mean(sim_list_2)\n",
    "    \n",
    "    sims_3 = list_sims_for_mean(sim_list_3)\n",
    "    \n",
    "    means_list = get_sim_means(sims_1, sims_2, sims_3)\n",
    "    \n",
    "    mean_cos_sims = []\n",
    "    \n",
    "    # make new list with comparison terms\n",
    "    \n",
    "    for sub_lists in sim_list_1:\n",
    "        \n",
    "        for i in sub_lists:\n",
    "            \n",
    "            local_sim_list = []\n",
    "            \n",
    "            local_sim_list.append(i[0])\n",
    "            \n",
    "            local_sim_list.append(i[1])\n",
    "            \n",
    "            mean_cos_sims.append(local_sim_list)\n",
    "            \n",
    "    # append mean cos sim values\n",
    "        \n",
    "    for i in range(len(mean_cos_sims)):\n",
    "        \n",
    "        mean_cos_sims[i].append(means_list[i])\n",
    "    \n",
    "    return mean_cos_sims\n",
    "                \n",
    "\n",
    "def text_prep_pipeline(subreddit_name, app_name, oa_script, oa_secret, pwd, vocab_number):\n",
    "    \n",
    "    subreddit = get_subreddit(subreddit_name, app_name, oa_script, oa_secret, pwd)\n",
    "        \n",
    "    subreddit_df = make_subreddit_df(subreddit)\n",
    "    \n",
    "    raw_corpus = join_corpus(subreddit_df)\n",
    "    \n",
    "    clean_corpus = clean_all_docs(raw_corpus)\n",
    "   \n",
    "    # get vocab takes raw corpus, NOT clean\n",
    "    vocab_df, total_words, unique_words = get_vocab(raw_corpus)\n",
    "    \n",
    "    top, bottom = get_top_and_bottom(vocab_df, vocab_number)\n",
    "    \n",
    "    return subreddit_df, raw_corpus, clean_corpus, vocab_df, top, bottom, total_words, unique_words\n",
    "\n",
    "\n",
    "def train_pipeline(docs, term_list_A, term_list_B):\n",
    "    \n",
    "    # train model\n",
    "    \n",
    "    trained_w2v_model = train_w2v_model(docs)\n",
    "    \n",
    "    term_cos_sims = comp_cos_sim_lists(trained_w2v_model, term_list_A, term_list_B)\n",
    "    \n",
    "    return trained_w2v_model, term_cos_sims\n",
    "\n",
    "\n",
    "def final_pipeline(subreddit_name, app_name, oa_script, oa_secret, pwd, vocab_number, term_list_A, term_list_B):\n",
    "    \n",
    "    #text_prep_pipeline(subreddit_name, app_name, oa_script, oa_secret, pwd, vocab_number)\n",
    "    # return subreddit_df, raw_corpus, clean_corpus, vocab_df, top, bottom\n",
    "    \n",
    "    subreddit_df, raw_corpus, clean_corpus, vocab_df, top, bottom, total_words, unique_words = text_prep_pipeline(subreddit_name, app_name, oa_script, oa_secret, pwd, vocab_number)\n",
    "    \n",
    "    # save data to file(s)\n",
    "    # prepare to export as CSVs\n",
    "    # first generate unique names\n",
    "    \n",
    "    subreddit_df_filename = app_name + '_subreddit.csv'\n",
    "    \n",
    "    raw_corpus_filename = app_name + '_raw_corpus.csv'\n",
    "    \n",
    "    clean_corpus_filename = app_name + '_clean_corpus.csv'\n",
    "    \n",
    "    vocab_df_filename = app_name + '_vocab.csv'\n",
    "    \n",
    "    # convert lists to DataFrames\n",
    "    \n",
    "    raw_corpus_df = pd.DataFrame(raw_corpus)\n",
    "    \n",
    "    clean_corpus_df = pd.DataFrame(clean_corpus)\n",
    "    \n",
    "    # export to CSVs using unique filenames\n",
    "    \n",
    "    subreddit_df.to_csv(subreddit_df_filename)\n",
    "    \n",
    "    raw_corpus_df.to_csv(raw_corpus_filename)\n",
    "    \n",
    "    clean_corpus_df.to_csv(clean_corpus_filename)\n",
    "    \n",
    "    vocab_df.to_csv(vocab_df_filename)\n",
    " \n",
    "    # train 3 models\n",
    "    \n",
    "    model_1, cos_sims_1 = train_pipeline(clean_corpus, term_list_A, term_list_B)\n",
    "    \n",
    "    model_2, cos_sims_2 = train_pipeline(clean_corpus, term_list_A, term_list_B)\n",
    "    \n",
    "    model_3, cos_sims_3 = train_pipeline(clean_corpus, term_list_A, term_list_B)\n",
    "    \n",
    "    final_sims_list = get_mean_cos_sim(cos_sims_1, cos_sims_2, cos_sims_3)\n",
    "    \n",
    "    final_sims_df = pd.DataFrame(final_sims_list)\n",
    "    \n",
    "    # generate unique names for the models & df\n",
    "    \n",
    "    model_1_filename = app_name + '_w2v_model_1.model'\n",
    "    \n",
    "    model_2_filename = app_name + '_w2v_model_2.model'\n",
    "    \n",
    "    model_3_filename = app_name + '_w2v_model_3.model'\n",
    "    \n",
    "    final_sims_filename = app_name + '_final_sims_list.csv'\n",
    "    \n",
    "    # save final cosine similarities & models to disk\n",
    "    \n",
    "    model_1.save(model_1_filename)\n",
    "    \n",
    "    model_2.save(model_2_filename)\n",
    "    \n",
    "    model_3.save(model_3_filename)\n",
    "    \n",
    "    final_sims_df.to_csv(final_sims_filename)\n",
    "    \n",
    "    # display results\n",
    "    \n",
    "    print('number of words used in training: \\n')\n",
    "    \n",
    "    print(total_words, '\\n')\n",
    "    \n",
    "    print('total unique vocabulary words: \\n')\n",
    "    \n",
    "    print(unique_words, '\\n')\n",
    "    \n",
    "    print('top vocabulary: \\n')\n",
    "    \n",
    "    print(top, '\\n')\n",
    "    \n",
    "    print('bottom vocabulary: \\n')\n",
    "    \n",
    "    print(bottom, '\\n')\n",
    "    \n",
    "    print('mean cosine similarities: \\n')\n",
    "    \n",
    "    print(final_sims_df)\n",
    "    \n",
    "    return model_1, model_2, model_3, final_sims_list, top, bottom\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
