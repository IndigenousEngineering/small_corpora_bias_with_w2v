{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TODO:\n",
    "\n",
    "function to clean all\n",
    "\n",
    "* returns 2 cleaned corpora (2 diff versions of text prep)\n",
    "\n",
    "function to train model on single corpus\n",
    "\n",
    "* visualize 2 ways (with/without words)\n",
    "* returns trained model\n",
    "\n",
    "function to test a list of words against a specific word\n",
    "\n",
    "* returns cos sims for each word relative to given word\n",
    "\n",
    "* returns as list and df\n",
    "\n",
    "etc/tbd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw\n",
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "from collections import Counter\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import PCA\n",
    "from matplotlib import pyplot\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate through reddit object, build dictionary, return dataframes\n",
    "# return titles&posts df, plus whole df\n",
    "\n",
    "def make_corpus(subreddit_object):\n",
    "    '''\n",
    "    takes subreddit api object,\n",
    "    returns 2 dataframes:\n",
    "    \n",
    "    * entire subreddit, all columns\n",
    "    * text corpus only\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    columns_dict = {'title': [],\n",
    "                   'url': [],\n",
    "                   'date': [],\n",
    "                   'score': [],\n",
    "                   'n_comments': [],\n",
    "                   'body': []}\n",
    "    \n",
    "    # build dictionary\n",
    "    \n",
    "    for post in subreddit_object:\n",
    "        \n",
    "        columns_dict['title'].append(post.title)\n",
    "        \n",
    "        columns_dict['url'].append(post.url)\n",
    "        \n",
    "        columns_dict['date'].append(post.created)\n",
    "        \n",
    "        columns_dict['score'].append(post.score)\n",
    "        \n",
    "        columns_dict['n_comments'].append(post.num_comments)\n",
    "        \n",
    "        columns_dict['body'].append(post.selftext)\n",
    "    \n",
    "    # convert to dataframe\n",
    "    \n",
    "    subreddit_df = pd.DataFrame(columns_dict)\n",
    "    \n",
    "    # split off text for df\n",
    "    \n",
    "    corpus_df = subreddit_df['title'] + subreddit_df['body']\n",
    "    \n",
    "    return subreddit_df, corpus_df\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text cleaning: 2 different versions\n",
    "# functions for standardization\n",
    "\n",
    "def light_clean(doc):\n",
    "    '''\n",
    "    lightly cleans input doc for\n",
    "    word embedding training:\n",
    "    \n",
    "    * tokenizes\n",
    "    * lowercase\n",
    "    * removes punctuation & non-alpha characters\n",
    "    \n",
    "    returns clean document.\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    # tokenize\n",
    "    \n",
    "    split_tokens = doc.split()\n",
    "    \n",
    "    # punctuation list\n",
    "    \n",
    "    punctuation = string.punctuation\n",
    "    \n",
    "    # set up regex filter\n",
    "    \n",
    "    rgx_punct_filter = re.compile('[%s]' % re.escape(punctuation))\n",
    "    \n",
    "    # apply filter\n",
    "    \n",
    "    no_punct_tokens = [rgx_punct_filter.sub('', char) for char in split_tokens]\n",
    "    \n",
    "    # alpha tokens only\n",
    "    \n",
    "    alpha_tokens = [t for t in no_punct_tokens if t.isalpha()]\n",
    "    \n",
    "    # lowercase\n",
    "    \n",
    "    lower_tokens = [t.lower() for t in alpha_tokens]\n",
    "    \n",
    "    return lower_tokens\n",
    "\n",
    "def super_clean(doc):\n",
    "    '''\n",
    "    cleans a single input document.\n",
    "    applies more processing than\n",
    "    light_clean():\n",
    "    \n",
    "    * tokenizes\n",
    "    * lowercase\n",
    "    * removes punctuation & numbers\n",
    "    * removes stopwords\n",
    "    \n",
    "    returns clean document.\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    # tokenize\n",
    "    \n",
    "    split_tokens = doc.split()\n",
    "    \n",
    "    # grab punctuation list\n",
    "    \n",
    "    punctuation = string.punctuation\n",
    "    \n",
    "    # setup re filter\n",
    "    \n",
    "    rgx_punct_filter = re.compile('[%s]' % re.escape(punctuation))\n",
    "    \n",
    "    # apply filter\n",
    "    \n",
    "    no_punct_tokens = [rgx_punct_filter.sub('', char) for char in split_tokens]\n",
    "    \n",
    "    # take out numbers & any other non-alpha characters\n",
    "    \n",
    "    alpha_tokens = [i for i in no_punct_tokens if i.isalpha()]\n",
    "    \n",
    "    # lowercase\n",
    "    \n",
    "    lower_tokens = [t.lower() for t in alpha_tokens]\n",
    "    \n",
    "    # get stop words\n",
    "    \n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    \n",
    "    # filter out stopwords\n",
    "    \n",
    "    go_tokens = [t for t in lower_tokens if not t in stop_words]\n",
    "    \n",
    "    # remove very short tokens\n",
    "    \n",
    "    clean_tokens = [word for word in go_tokens if len(word) > 1]\n",
    "    \n",
    "    return clean_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def clean_all_docs(docs):\n",
    "    '''\n",
    "    takes corpus (list) as input\n",
    "    iterates through input texts\n",
    "    applies 2 different text cleaning functions\n",
    "    appends cleaned documents to 2 seperate lists\n",
    "    returns 2 lists of clean texts, and 2\n",
    "    dataframes with vocabulary counts\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    light_clean_docs = []\n",
    "    \n",
    "    light_clean_vocab = []\n",
    "    \n",
    "    super_clean_docs = []\n",
    "    \n",
    "    super_clean_vocab = []\n",
    "    \n",
    "    for doc in docs:\n",
    "        \n",
    "        # eliminate empty rows\n",
    "        \n",
    "        if doc:\n",
    "        \n",
    "            clean_doc_1 = light_clean(doc)\n",
    "            \n",
    "            clean_doc_2 = super_clean(doc)\n",
    "        \n",
    "            light_clean_docs.append(clean_doc_1)\n",
    "            \n",
    "            super_clean_docs.append(clean_doc_2)\n",
    "            \n",
    "    # get vocabulary\n",
    "    \n",
    "    for doc in light_clean_docs:\n",
    "        \n",
    "        for word in doc:\n",
    "            \n",
    "            light_clean_vocab.append(word)\n",
    "            \n",
    "    for doc in super_clean_docs:\n",
    "        \n",
    "        for word in doc:\n",
    "            \n",
    "            super_clean_vocab.append(word)\n",
    "    \n",
    "    # get word counts\n",
    "    \n",
    "    light_clean_counts = Counter(light_clean_vocab)\n",
    "    \n",
    "    super_clean_counts = Counter(super_clean_vocab)\n",
    "    \n",
    "    # get light clean text vocab to dataframe\n",
    "    \n",
    "    lc_vocab_df = pd.DataFrame.from_dict(light_clean_counts, orient='index')\n",
    "    \n",
    "    # sort by most frequent words\n",
    "    \n",
    "    lc_vocab_df = lc_vocab_df.sort_values(by=[0], ascending=False)\n",
    "    \n",
    "    # index\n",
    "    \n",
    "    light_clean_vocab = lc_vocab_df.reset_index()\n",
    "    \n",
    "    # get super clean text vocab\n",
    "    \n",
    "    sc_vocab_df = pd.DataFrame.from_dict(super_clean_counts, orient='index')\n",
    "    \n",
    "    sc_vocab_df = sc_vocab_df.sort_values(by=0, ascending=False)\n",
    "    \n",
    "    super_clean_vocab = sc_vocab_df.reset_index()\n",
    "     \n",
    "            \n",
    "    return light_clean_docs, light_clean_vocab, super_clean_docs, super_clean_vocab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some words from Maisie Williams \n",
      "\n",
      "This sadly happens all to often. \n",
      "\n",
      "This can never go away, this needs to continue to get posted over and over and over again. This should never be forgotten \n",
      "\n",
      "About abortion \n",
      "\n",
      "Trump Inauguration (top) vs. Women's March (bottom) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# test on a subreddit\n",
    "# will use r/Feminism for this run\n",
    "\n",
    "with open('r_feminism_script.txt') as f:\n",
    "    \n",
    "    feminism_script = f.read().strip()\n",
    "\n",
    "with open('r_feminism_secret.txt') as f:\n",
    "    \n",
    "    feminism_secret = f.read().strip()\n",
    "\n",
    "with open('reddit_id.txt') as f:\n",
    "    \n",
    "    pw = f.read().strip()\n",
    "    \n",
    "# get reddit object\n",
    "    \n",
    "reddit = praw.Reddit(client_id=feminism_script,\n",
    "                    client_secret=feminism_secret,\n",
    "                    user_agent='feminism',\n",
    "                    username='neurodivergent_ai',\n",
    "                    password=pw)\n",
    "\n",
    "# get subreddit\n",
    "\n",
    "feminism = reddit.subreddit('Feminism')\n",
    "\n",
    "# get top posts\n",
    "\n",
    "r_feminism = feminism.top(limit=1000)\n",
    "\n",
    "for post in feminism.top(limit=5):\n",
    "    \n",
    "    print(post.title, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_feminism_df, r_feminism_corpus = make_corpus(r_feminism)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "full dataframe .head(): \n",
      "\n",
      "                                               title  \\\n",
      "0                    Some words from Maisie Williams   \n",
      "1                   This sadly happens all to often.   \n",
      "2  This can never go away, this needs to continue...   \n",
      "3                                     About abortion   \n",
      "4  Trump Inauguration (top) vs. Women's March (bo...   \n",
      "\n",
      "                                   url          date  score  n_comments body  \n",
      "0   https://i.redd.it/ka0kgautv32z.jpg  1.496819e+09  10020         980       \n",
      "1      https://i.imgur.com/0lLo035.jpg  1.509409e+09   5992         392       \n",
      "2  https://i.redd.it/ar0tu4fi2jt01.jpg  1.524461e+09   3959         180       \n",
      "3  https://i.redd.it/k4at3i36c6121.jpg  1.543484e+09   3234         213       \n",
      "4       http://i.imgur.com/ivzyXdP.png  1.485059e+09   2727         166        \n",
      "\n",
      "text & title corpus .head(): \n",
      "\n",
      "0                      Some words from Maisie Williams\n",
      "1                     This sadly happens all to often.\n",
      "2    This can never go away, this needs to continue...\n",
      "3                                       About abortion\n",
      "4    Trump Inauguration (top) vs. Women's March (bo...\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print('full dataframe .head(): \\n')\n",
    "print(r_feminism_df.head(), '\\n')\n",
    "print('text & title corpus .head(): \\n')\n",
    "print(r_feminism_corpus.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return light_clean_docs, light_clean_vocab, super_clean_docs, super_clean_vocab\n",
    "\n",
    "feminism_lc_docs, feminism_lc_vocab, feminism_sc_docs, feminism_sc_vocab = clean_all_docs(r_feminism_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
