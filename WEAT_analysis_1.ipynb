{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/keras/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import praw\n",
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get reddit secret & script i've saved as .txt in the working directory\n",
    "\n",
    "with open('reddit_oa_script.txt') as f:\n",
    "    \n",
    "    reddit_script = f.read().strip()\n",
    "    \n",
    "with open('reddit_oa_secret.txt') as f:\n",
    "    \n",
    "    reddit_secret = f.read().strip()\n",
    "    \n",
    "with open('reddit_id.txt') as f:\n",
    "    \n",
    "    pw = f.read().strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "# user_agent is the name of the app\n",
    "\n",
    "reddit  = praw.Reddit(client_id=reddit_script,\n",
    "                          client_secret=reddit_secret,\n",
    "                          user_agent='get_corpora',\n",
    "                          username='neurodivergent_ai',\n",
    "                          password=pw)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the subreddit\n",
    "\n",
    "subreddit_1 = reddit.subreddit('The_Donald')\n",
    "\n",
    "# get the top (upvoted) 1000 threads\n",
    "\n",
    "subreddit_1_top = subreddit_1.top(limit=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONALD J. TRUMP DECLARED THE WINNER! \n",
      "\n",
      "I’m Donald J. Trump and I'm Your Next President of the United States. \n",
      "\n",
      "The New Algorithm Was Designed To Keep President Trump From The Front Page... But Sadly, That Won't Happen. \n",
      "\n",
      "CNN will soon be #1...when searching for the term \"Fake News\". Congratulations to CNN on this major achievement. \n",
      "\n",
      "ALL CELEBRITIES THAT VOWED TO LEAVE THE U.S.A. IF TRUMP WINS, WE NOW DEMAND YOU TO FOLLOW THROUGH!!!! OUT! OUT! OUT! \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# get the top 5 submissions to check\n",
    "\n",
    "for post in subreddit_1.top(limit=5):\n",
    "    \n",
    "    print(post.title, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a dataset\n",
    "\n",
    "# choose information to gather\n",
    "\n",
    "columns_dict = {'title':[],\n",
    "               'id':[],\n",
    "               'url':[],\n",
    "               'date':[],\n",
    "               'score':[],\n",
    "               'num_comments':[],\n",
    "               'body':[]}\n",
    "\n",
    "# build a dictionary\n",
    "\n",
    "for post in subreddit_1_top:\n",
    "    \n",
    "    columns_dict['title'].append(post.title)\n",
    "    \n",
    "    columns_dict['id'].append(post.id)\n",
    "    \n",
    "    columns_dict['url'].append(post.url)\n",
    "    \n",
    "    columns_dict['date'].append(post.created)\n",
    "    \n",
    "    columns_dict['score'].append(post.score)\n",
    "    \n",
    "    columns_dict['num_comments'].append(post.num_comments)\n",
    "    \n",
    "    columns_dict['body'].append(post.selftext)\n",
    "\n",
    "# convert dictionary to a dataframe\n",
    "\n",
    "subreddit_1_df = pd.DataFrame(columns_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>id</th>\n",
       "      <th>url</th>\n",
       "      <th>date</th>\n",
       "      <th>score</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DONALD J. TRUMP DECLARED THE WINNER!</td>\n",
       "      <td>5bzjv5</td>\n",
       "      <td>https://www.reddit.com/r/The_Donald/comments/5...</td>\n",
       "      <td>1.478706e+09</td>\n",
       "      <td>66714</td>\n",
       "      <td>12097</td>\n",
       "      <td>#Meme Magic is REAL.\\n\\n#Donald J. Trump is th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I’m Donald J. Trump and I'm Your Next Presiden...</td>\n",
       "      <td>4uxdbn</td>\n",
       "      <td>https://www.reddit.com/r/The_Donald/comments/4...</td>\n",
       "      <td>1.469687e+09</td>\n",
       "      <td>42118</td>\n",
       "      <td>20806</td>\n",
       "      <td>Hello The_Donald readers and the entire Reddit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The New Algorithm Was Designed To Keep Preside...</td>\n",
       "      <td>5gvy1j</td>\n",
       "      <td>https://i.sli.mg/gu0oHA.png</td>\n",
       "      <td>1.481092e+09</td>\n",
       "      <td>40966</td>\n",
       "      <td>835</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CNN will soon be #1...when searching for the t...</td>\n",
       "      <td>5jt9xs</td>\n",
       "      <td>https://i.redd.it/7jeyhzgku65y.png</td>\n",
       "      <td>1.482472e+09</td>\n",
       "      <td>37276</td>\n",
       "      <td>724</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ALL CELEBRITIES THAT VOWED TO LEAVE THE U.S.A....</td>\n",
       "      <td>5bz5ds</td>\n",
       "      <td>https://www.reddit.com/r/The_Donald/comments/5...</td>\n",
       "      <td>1.478702e+09</td>\n",
       "      <td>36614</td>\n",
       "      <td>1685</td>\n",
       "      <td>Now that we have proven our commitment to the ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title      id  \\\n",
       "0               DONALD J. TRUMP DECLARED THE WINNER!  5bzjv5   \n",
       "1  I’m Donald J. Trump and I'm Your Next Presiden...  4uxdbn   \n",
       "2  The New Algorithm Was Designed To Keep Preside...  5gvy1j   \n",
       "3  CNN will soon be #1...when searching for the t...  5jt9xs   \n",
       "4  ALL CELEBRITIES THAT VOWED TO LEAVE THE U.S.A....  5bz5ds   \n",
       "\n",
       "                                                 url          date  score  \\\n",
       "0  https://www.reddit.com/r/The_Donald/comments/5...  1.478706e+09  66714   \n",
       "1  https://www.reddit.com/r/The_Donald/comments/4...  1.469687e+09  42118   \n",
       "2                        https://i.sli.mg/gu0oHA.png  1.481092e+09  40966   \n",
       "3                 https://i.redd.it/7jeyhzgku65y.png  1.482472e+09  37276   \n",
       "4  https://www.reddit.com/r/The_Donald/comments/5...  1.478702e+09  36614   \n",
       "\n",
       "   num_comments                                               body  \n",
       "0         12097  #Meme Magic is REAL.\\n\\n#Donald J. Trump is th...  \n",
       "1         20806  Hello The_Donald readers and the entire Reddit...  \n",
       "2           835                                                     \n",
       "3           724                                                     \n",
       "4          1685  Now that we have proven our commitment to the ...  "
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subreddit_1_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a copy to work on\n",
    "\n",
    "reddit_df = subreddit_1_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grab texts to build into corpus\n",
    "\n",
    "titles_df = reddit_df['title']\n",
    "\n",
    "posts_df = reddit_df['body']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                 DONALD J. TRUMP DECLARED THE WINNER!\n",
       "1    I’m Donald J. Trump and I'm Your Next Presiden...\n",
       "2    The New Algorithm Was Designed To Keep Preside...\n",
       "3    CNN will soon be #1...when searching for the t...\n",
       "4    ALL CELEBRITIES THAT VOWED TO LEAVE THE U.S.A....\n",
       "5                             IMMINENT VICTORY THREAD.\n",
       "6    Hey admins, we found a picture of your wife's ...\n",
       "7             Oh yeah, nothing fucky going on here :^)\n",
       "8                     Can't Stump your President Trump\n",
       "9    It's official: Trump will become the first U.S...\n",
       "Name: title, dtype: object"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titles_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    #Meme Magic is REAL.\\n\\n#Donald J. Trump is th...\n",
       "1    Hello The_Donald readers and the entire Reddit...\n",
       "2                                                     \n",
       "3                                                     \n",
       "4    Now that we have proven our commitment to the ...\n",
       "5    #THIS IS NOT A DRILL. WE ARE ACTUALLY GOING TO...\n",
       "6                                                     \n",
       "7                                                     \n",
       "8                                                     \n",
       "9                                                     \n",
       "Name: body, dtype: object"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "posts_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Now', 'that', 'we', 'have', 'proven', 'our', 'commitment', 'to', 'the', 'cause,', 'now', 'that', 'we', 'have', 'voiced', 'and', 'acted', 'on', 'our', 'patriotism,', 'now', 'that', 'against', 'all', 'odds', 'and', 'constant', 'criticism', 'with', 'the', 'world', 'against', 'us,', 'we', 'have', 'won,', 'and', 'we', 'call', 'for', 'this:', '#All', 'of', 'you', 'elitist', 'hollywood', 'celebrities', 'who', 'vowed', 'to', 'betray', 'your', 'country', 'and', 'leave,', 'to', 'abandon', 'and', 'shun', 'those', 'who', 'you', 'spent', 'your', 'careers', 'milking', 'for', 'your', 'posh', 'lifestyles,', 'we', 'ask,', 'nay,', 'demand', 'that', 'you', 'follow', 'through', 'on', 'your', 'claims', 'to', 'leave', 'the', 'country.', 'We', 'as', 'Americans', 'have', 'spoken,', 'and', 'you', 'brushed', 'us', 'and', 'our', 'movement', 'off,', 'as', 'if', 'you', 'were', 'somehow', 'above', 'us,', 'forgetting', 'that', 'it', 'was', 'us', 'who', 'made', 'you.', '**FUCK', 'YOU!**', 'You', 'think', 'you', 'are', 'too', 'good', 'for', 'us,', 'well,', \"we're\", 'too', 'good', 'for', 'you.', 'So', 'follow', 'through', 'on', 'your', 'vapid', 'and', 'empty', '\"threat\"', 'as', 'you', 'mistakenly', 'and', 'pridefully', 'consider', 'it', 'to', 'be,', 'and', '**GET.', 'THE.', 'FUCK.', 'OUT.**', 'Your', 'fame', 'and', 'celebrity', 'status', 'are', 'despicable,', 'or', 'shall', 'I', 'say,', 'deplorable.', 'You', 'are', 'not', 'better', 'than', 'us,', 'you', 'are', 'not', 'more', 'important', 'than', 'us,', 'you', 'would', 'be', 'nothing', 'without', 'us.', 'How', 'dare', 'you', 'pretend', 'that', 'you', 'are', 'above', 'us.', '_______', 'So', 'all', 'of', 'you', 'on', 'the', 'list', 'below,', 'we', 'demand', 'you', 'leave.', 'ASAP!!!', '*', 'Amy', '(unfunny)', 'schumer', '*', 'Lena', 'Dunham', '*', 'Barbra', 'Streisand', '*', 'Bryan', 'Cranston', '*', 'Miley', 'Cyrus', '*', 'Amy', 'Schumer', '*', 'Jon', 'Stewart', '*', 'Cher', '*', 'Chelsea', 'Handler', '*', 'Samuel', 'L.', 'Jackson', '*', 'Whoopi', 'Goldberg', '*', 'Neve', 'Campbell', '*', 'Keegan-Michael', 'Key', '*', 'George', 'Lopez', '*', 'Ne-Yo', '*', 'Rev.', 'Al', 'Sharpton', '*', 'Raven-Symoné', '#BUH-BYE.', '#Be', 'assured,', 'YOU', 'WILL', 'NOT', 'BE', 'MISSED.', 'Love,', 'The', 'Basket', 'of', 'Deplorables']\n"
     ]
    }
   ],
   "source": [
    "# experiment\n",
    "# need to toeknize, etc\n",
    "\n",
    "text = posts_df[4].split()\n",
    "\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the text needs to be cleaned. lots of stopwords, punctuation, & non-alpha characters. case needs to be lowered etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(doc):\n",
    "    '''\n",
    "    tokenizes input text, converts to lowercase,\n",
    "    removes punctuation/non-alphabetic characters\n",
    "    and stopwords. returns clean document.\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    # tokenize\n",
    "    \n",
    "    split_tokens = doc.split()\n",
    "    \n",
    "    # grab punctuation list\n",
    "    \n",
    "    punctuation = string.punctuation\n",
    "    \n",
    "    # set up regex \n",
    "    \n",
    "    punct_filter = re.compile('[%s]' % re.escape(punctuation))\n",
    "    \n",
    "    # filter out punctuation\n",
    "    \n",
    "    no_punct_tokens = [punct_filter.sub('', char) for char in split_tokens]\n",
    "    \n",
    "    # remove non-alphabetic tokens\n",
    "    \n",
    "    alpha_tokens = [t for t in no_punct_tokens if t.isalpha()]\n",
    "    \n",
    "    # lowercase\n",
    "    \n",
    "    lower_tokens = [t.lower() for t in alpha_tokens]\n",
    "    \n",
    "    # get stop words\n",
    "    \n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    \n",
    "    # filter out stopwords\n",
    "    \n",
    "    go_tokens = [t for t in lower_tokens if not t in stop_words]\n",
    "    \n",
    "    # remove very short tokens\n",
    "    \n",
    "    clean_tokens = [word for word in go_tokens if len(word) > 1]\n",
    "    \n",
    "    return clean_tokens\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['proven',\n",
       " 'commitment',\n",
       " 'cause',\n",
       " 'voiced',\n",
       " 'acted',\n",
       " 'patriotism',\n",
       " 'odds',\n",
       " 'constant',\n",
       " 'criticism',\n",
       " 'world',\n",
       " 'us',\n",
       " 'call',\n",
       " 'elitist',\n",
       " 'hollywood',\n",
       " 'celebrities',\n",
       " 'vowed',\n",
       " 'betray',\n",
       " 'country',\n",
       " 'leave',\n",
       " 'abandon',\n",
       " 'shun',\n",
       " 'spent',\n",
       " 'careers',\n",
       " 'milking',\n",
       " 'posh',\n",
       " 'lifestyles',\n",
       " 'ask',\n",
       " 'nay',\n",
       " 'demand',\n",
       " 'follow',\n",
       " 'claims',\n",
       " 'leave',\n",
       " 'country',\n",
       " 'americans',\n",
       " 'spoken',\n",
       " 'brushed',\n",
       " 'us',\n",
       " 'movement',\n",
       " 'somehow',\n",
       " 'us',\n",
       " 'forgetting',\n",
       " 'us',\n",
       " 'made',\n",
       " 'fuck',\n",
       " 'think',\n",
       " 'good',\n",
       " 'us',\n",
       " 'well',\n",
       " 'good',\n",
       " 'follow',\n",
       " 'vapid',\n",
       " 'empty',\n",
       " 'threat',\n",
       " 'mistakenly',\n",
       " 'pridefully',\n",
       " 'consider',\n",
       " 'get',\n",
       " 'fuck',\n",
       " 'fame',\n",
       " 'celebrity',\n",
       " 'status',\n",
       " 'despicable',\n",
       " 'shall',\n",
       " 'say',\n",
       " 'deplorable',\n",
       " 'better',\n",
       " 'us',\n",
       " 'important',\n",
       " 'us',\n",
       " 'would',\n",
       " 'nothing',\n",
       " 'without',\n",
       " 'us',\n",
       " 'dare',\n",
       " 'pretend',\n",
       " 'us',\n",
       " 'list',\n",
       " 'demand',\n",
       " 'leave',\n",
       " 'asap',\n",
       " 'amy',\n",
       " 'unfunny',\n",
       " 'schumer',\n",
       " 'lena',\n",
       " 'dunham',\n",
       " 'barbra',\n",
       " 'streisand',\n",
       " 'bryan',\n",
       " 'cranston',\n",
       " 'miley',\n",
       " 'cyrus',\n",
       " 'amy',\n",
       " 'schumer',\n",
       " 'jon',\n",
       " 'stewart',\n",
       " 'cher',\n",
       " 'chelsea',\n",
       " 'handler',\n",
       " 'samuel',\n",
       " 'jackson',\n",
       " 'whoopi',\n",
       " 'goldberg',\n",
       " 'neve',\n",
       " 'campbell',\n",
       " 'keeganmichael',\n",
       " 'key',\n",
       " 'george',\n",
       " 'lopez',\n",
       " 'neyo',\n",
       " 'rev',\n",
       " 'al',\n",
       " 'sharpton',\n",
       " 'ravensymoné',\n",
       " 'buhbye',\n",
       " 'assured',\n",
       " 'missed',\n",
       " 'love',\n",
       " 'basket',\n",
       " 'deplorables']"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test\n",
    "\n",
    "raw_text = posts_df[4]\n",
    "\n",
    "clean_text(raw_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_all_texts(texts):\n",
    "    '''\n",
    "    iterate through a list of texts\n",
    "    apply clean_text() function to each\n",
    "    append each clean text to a list\n",
    "    return the list of cleaned texts\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    cleaned_texts = []\n",
    "    \n",
    "    for text in texts:\n",
    "        \n",
    "        text = clean_text(text)\n",
    "        \n",
    "        cleaned_texts.append(text)\n",
    "        \n",
    "    return cleaned_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_titles = clean_all_texts(titles)\n",
    "\n",
    "clean_posts = clean_all_texts(posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['celebrities', 'vowed', 'leave', 'usa', 'trump', 'wins', 'demand', 'follow']\n",
      "\n",
      "\n",
      "['proven', 'commitment', 'cause', 'voiced', 'acted', 'patriotism', 'odds', 'constant', 'criticism', 'world', 'us', 'call', 'elitist', 'hollywood', 'celebrities', 'vowed', 'betray', 'country', 'leave', 'abandon', 'shun', 'spent', 'careers', 'milking', 'posh', 'lifestyles', 'ask', 'nay', 'demand', 'follow', 'claims', 'leave', 'country', 'americans', 'spoken', 'brushed', 'us', 'movement', 'somehow', 'us', 'forgetting', 'us', 'made', 'fuck', 'think', 'good', 'us', 'well', 'good', 'follow', 'vapid', 'empty', 'threat', 'mistakenly', 'pridefully', 'consider', 'get', 'fuck', 'fame', 'celebrity', 'status', 'despicable', 'shall', 'say', 'deplorable', 'better', 'us', 'important', 'us', 'would', 'nothing', 'without', 'us', 'dare', 'pretend', 'us', 'list', 'demand', 'leave', 'asap', 'amy', 'unfunny', 'schumer', 'lena', 'dunham', 'barbra', 'streisand', 'bryan', 'cranston', 'miley', 'cyrus', 'amy', 'schumer', 'jon', 'stewart', 'cher', 'chelsea', 'handler', 'samuel', 'jackson', 'whoopi', 'goldberg', 'neve', 'campbell', 'keeganmichael', 'key', 'george', 'lopez', 'neyo', 'rev', 'al', 'sharpton', 'ravensymoné', 'buhbye', 'assured', 'missed', 'love', 'basket', 'deplorables']\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "\n",
    "print(clean_titles[4])\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "print(clean_posts[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['donald', 'trump', 'declared', 'winner'] \n",
      "\n",
      "['donald', 'trump', 'im', 'next', 'president', 'united', 'states'] \n",
      "\n",
      "['new', 'algorithm', 'designed', 'keep', 'president', 'trump', 'front', 'page', 'sadly', 'wont', 'happen'] \n",
      "\n",
      "['cnn', 'soon', 'searching', 'term', 'fake', 'news', 'congratulations', 'cnn', 'major', 'achievement'] \n",
      "\n",
      "['celebrities', 'vowed', 'leave', 'usa', 'trump', 'wins', 'demand', 'follow'] \n",
      "\n",
      "['imminent', 'victory', 'thread'] \n",
      "\n",
      "['hey', 'admins', 'found', 'picture', 'wifes', 'boyfriends', 'president'] \n",
      "\n",
      "['oh', 'yeah', 'nothing', 'fucky', 'going'] \n",
      "\n",
      "['cant', 'stump', 'president', 'trump'] \n",
      "\n",
      "['official', 'trump', 'become', 'first', 'us', 'president', 'support', 'gay', 'marriage', 'starting', 'inauguration', 'day', 'shame', 'reached', 'rall'] \n",
      "\n",
      "['fuck', 'reddit', 'admins', 'like', 'redit', 'try', 'much', 'like', 'cant', 'stop', 'portrait', 'getting', 'upvotes', 'deserves'] \n",
      "\n",
      "['reddit', 'cuck', 'admins', 'take', 'official', 'portrait', 'president', 'upvoted', 'post', 'reddits', 'history'] \n",
      "\n",
      "['press', 'pay', 'respect'] \n",
      "\n",
      "['youtube', 'removed', 'countless', 'copies', 'video', 'bunch', 'racists', 'beating', 'trump', 'voter', 'issued', 'strikes', 'accounts', 'uploaded', 'posting', 'every', 'day', 'racists', 'identified', 'punished'] \n",
      "\n",
      "['hey', 'rall'] \n",
      "\n",
      "['fired'] \n",
      "\n",
      "['little', 'something', 'reddit', 'admins', 'get', 'used', 'looking', 'around', 'next', 'years'] \n",
      "\n",
      "['official', 'portrait', 'president', 'upvotednot', 'post', 'reddits', 'history'] \n",
      "\n",
      "['trump'] \n",
      "\n",
      "['united', 'states', 'trump'] \n",
      "\n",
      "['hero', 'stopped', 'terrorist', 'attack', 'today', 'ohio', 'state'] \n",
      "\n",
      "['portrait', 'powerful', 'man', 'free', 'world'] \n",
      "\n",
      "['reddit', 'voting', 'algorithm', 'changed', 'picture', 'droniest', 'president', 'ever', 'highest', 'voted', 'shitpost', 'time'] \n",
      "\n",
      "['heard', 'reddit', 'admins', 'hate', 'photo', 'know', 'centipedes'] \n",
      "\n",
      "['top', 'god', 'emperor'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# get the first 25 titles\n",
    "\n",
    "counter = 0\n",
    "\n",
    "for i in clean_titles:\n",
    "    \n",
    "    if counter < 25:\n",
    "        \n",
    "        print(i, '\\n')\n",
    "        \n",
    "        counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['meme', 'magic', 'real', 'donald', 'trump', 'next', 'president', 'united', 'states', 'america', 'let', 'tendies', 'hit', 'floor', 'donald', 'trump', 'proved', 'entire', 'world', 'wrong', 'insurmountable', 'odds', 'corruption', 'media', 'sabotage', 'party', 'crush', 'establishment', 'machine', 'donald', 'trump', 'feel', 'centipedes', 'god', 'emperor', 'said', 'would', 'get', 'tired', 'winning', 'tired', 'winning', 'yet', 'feel', 'vindicated', 'centipedes', 'nothing', 'future', 'history', 'part', 'history', 'thedonald', 'part', 'history', 'knew', 'hearts', 'one', 'believed', 'us', 'always', 'knew', 'truth', 'brakes', 'train', 'trump', 'wins'] \n",
      "\n",
      "['hello', 'thedonald', 'readers', 'entire', 'reddit', 'community', 'going', 'huge', 'im', 'looking', 'forward', 'answering', 'questions', 'im', 'flight', 'visit', 'great', 'people', 'toledo', 'oh', 'internet', 'connection', 'might', 'spotty', 'promise', 'ill', 'answer', 'questions', 'want', 'big', 'things', 'america', 'president', 'make', 'america', 'great', 'back', 'pm', 'et', 'update', 'proof', 'looking', 'forward', 'answering', 'questions', 'still', 'air', 'way', 'ohio', 'great', 'time', 'answering', 'questions', 'thank'] \n",
      "\n",
      "[] \n",
      "\n",
      "[] \n",
      "\n",
      "['proven', 'commitment', 'cause', 'voiced', 'acted', 'patriotism', 'odds', 'constant', 'criticism', 'world', 'us', 'call', 'elitist', 'hollywood', 'celebrities', 'vowed', 'betray', 'country', 'leave', 'abandon', 'shun', 'spent', 'careers', 'milking', 'posh', 'lifestyles', 'ask', 'nay', 'demand', 'follow', 'claims', 'leave', 'country', 'americans', 'spoken', 'brushed', 'us', 'movement', 'somehow', 'us', 'forgetting', 'us', 'made', 'fuck', 'think', 'good', 'us', 'well', 'good', 'follow', 'vapid', 'empty', 'threat', 'mistakenly', 'pridefully', 'consider', 'get', 'fuck', 'fame', 'celebrity', 'status', 'despicable', 'shall', 'say', 'deplorable', 'better', 'us', 'important', 'us', 'would', 'nothing', 'without', 'us', 'dare', 'pretend', 'us', 'list', 'demand', 'leave', 'asap', 'amy', 'unfunny', 'schumer', 'lena', 'dunham', 'barbra', 'streisand', 'bryan', 'cranston', 'miley', 'cyrus', 'amy', 'schumer', 'jon', 'stewart', 'cher', 'chelsea', 'handler', 'samuel', 'jackson', 'whoopi', 'goldberg', 'neve', 'campbell', 'keeganmichael', 'key', 'george', 'lopez', 'neyo', 'rev', 'al', 'sharpton', 'ravensymoné', 'buhbye', 'assured', 'missed', 'love', 'basket', 'deplorables'] \n",
      "\n",
      "['drill', 'actually', 'going', 'win', 'sjws', 'suck', 'dick'] \n",
      "\n",
      "[] \n",
      "\n",
      "[] \n",
      "\n",
      "[] \n",
      "\n",
      "[] \n",
      "\n",
      "[] \n",
      "\n",
      "[] \n",
      "\n",
      "[] \n",
      "\n",
      "[] \n",
      "\n",
      "[] \n",
      "\n",
      "[] \n",
      "\n",
      "[] \n",
      "\n",
      "[] \n",
      "\n",
      "[] \n",
      "\n",
      "[] \n",
      "\n",
      "[] \n",
      "\n",
      "[] \n",
      "\n",
      "[] \n",
      "\n",
      "[] \n",
      "\n",
      "[] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# get the first 25 posts\n",
    "\n",
    "counter = 0\n",
    "\n",
    "for i in clean_posts:\n",
    "    \n",
    "    if counter < 25:\n",
    "        \n",
    "        print(i, '\\n')\n",
    "        \n",
    "        counter += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "there are no missing lines in `clean_titles`, since every post must at least have a title. some posts, however, consist only of the title (with no body). \n",
    "\n",
    "because of this, there are a number of empty rows in `clean_posts`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
